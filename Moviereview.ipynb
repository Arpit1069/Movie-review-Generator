{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arpit1069/Movie-review-Generator/blob/main/Moviereview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4ZLZZ-629MY"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import random\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsb2rgIy29Ma"
      },
      "source": [
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "    \"\"\"\n",
        "    Mask the upper half of the dot product matrix in self attention.\n",
        "    This prevents flow of information from future tokens to current token.\n",
        "    1's in the lower triangle, counting from the lower right corner.\n",
        "    \"\"\"\n",
        "    i = tf.range(n_dest)[:, None]\n",
        "    j = tf.range(n_src)\n",
        "    m = i >= j - n_src + n_dest\n",
        "    mask = tf.cast(m, dtype)\n",
        "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "    mult = tf.concat(\n",
        "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
        "    )\n",
        "    return tf.tile(mask, mult)\n",
        "\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
        "        attention_output = self.dropout1(attention_output)\n",
        "        out1 = self.layernorm1(inputs + attention_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.layernorm2(out1 + ffn_output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGmuYX2B29Md"
      },
      "source": [
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IaJqO43cjoX"
      },
      "source": [
        "class TokenPositionAndSentimentEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, sentiment_size, embed_dim):\n",
        "        super(TokenPositionAndSentimentEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.sentiment_emb = layers.Embedding(input_dim=sentiment_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x, s):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        sentiment = s\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        sentiments= self.sentiment_emb(sentiment)\n",
        "        return x + positions + sentiments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkZVKXks6VFD"
      },
      "source": [
        "vocab_size = 20000  # Only consider the top 20k words\n",
        "sentiment_size = 2  # Positive Negative movie reviews\n",
        "maxlen = 80  # Max sequence size\n",
        "\n",
        "embed_dim = 256  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    inputs_tokens = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
        "    inputs_sentiments = layers.Input(shape=(1,), dtype=tf.int32)\n",
        "    #embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "    embedding_layer = TokenPositionAndSentimentEmbedding(maxlen, vocab_size, sentiment_size, embed_dim)\n",
        "    x = embedding_layer(inputs_tokens,inputs_sentiments)\n",
        "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
        "    x = transformer_block(x)\n",
        "    outputs = layers.Dense(vocab_size)(x)\n",
        "    model = keras.Model(inputs=[inputs_tokens,inputs_sentiments], outputs=[outputs, x])\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    model.compile(\n",
        "        \"adam\", loss=[loss_fn, None],\n",
        "    )  # No loss and optimization based on word embeddings from transformer block\n",
        "    return model\n",
        "my_model=create_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKrvBhm529Mh",
        "outputId": "de8eb2bc-2235-41d5-bde2-7e14397d2190"
      },
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  10.2M      0  0:00:07  0:00:07 --:--:-- 17.7M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNOVr2fAXHyZ",
        "outputId": "c6bee6b9-3d76-4b32-c1f1-1f268e7a2f13"
      },
      "source": [
        "batch_size = 128\n",
        "\n",
        "# The dataset contains each review in a separate text file\n",
        "# The text files are present in four different folders\n",
        "# Create a list all files\n",
        "filenames_positive = []\n",
        "filenames_negative = []\n",
        "directories_positive = [\n",
        "    \"aclImdb/train/pos\",\n",
        "    \"aclImdb/test/pos\",\n",
        "]\n",
        "\n",
        "directories_negative = [\n",
        "    \"aclImdb/train/neg\",\n",
        "    \"aclImdb/test/neg\",\n",
        "]\n",
        "for dir in directories_positive:\n",
        "    for f in os.listdir(dir):\n",
        "        filenames_positive.append(os.path.join(dir, f))\n",
        "for dir in directories_negative:\n",
        "    for f in os.listdir(dir):\n",
        "        filenames_negative.append(os.path.join(dir, f))\n",
        "print(f\"Total number of positive review files: {len(filenames_positive)}\")\n",
        "print(f\"Total number of negative review files: {len(filenames_negative)}\")\n",
        "\n",
        "all_text_ds_raw = tf.data.TextLineDataset([filenames_positive,filenames_negative])\n",
        "all_text_ds_raw = all_text_ds_raw.batch(batch_size)\n",
        "\n",
        "text_pos_ds_raw = tf.data.TextLineDataset(filenames_positive)\n",
        "text_neg_ds_raw = tf.data.TextLineDataset(filenames_negative)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of positive review files: 25000\n",
            "Total number of negative review files: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ9JCApOXcxi"
      },
      "source": [
        "def custom_standardization(input_string):\n",
        "    \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n",
        "    lowercased = tf.strings.lower(input_string)\n",
        "    stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
        "\n",
        "# Create a vectorization layer and adapt it to the text\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size - 1,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=maxlen,\n",
        ")\n",
        "vectorize_layer.adapt(all_text_ds_raw)\n",
        "vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbKYXg1hWsGx",
        "outputId": "4942c26f-9b38-44b2-eaea-e873369529e3"
      },
      "source": [
        "print(\"vocab has the \", len(vocab),\" entries\")\n",
        "print(\"vocab has the following first 10 entries\")\n",
        "for word in range(10):\n",
        "  print(word, \" represents the word: \", vocab[word])\n",
        "\n",
        "for X in all_text_ds_raw.take(1):\n",
        "  print(\" Given raw data (text): \", X[0].numpy() )\n",
        "  print(\" Tokenized and Transformed to a vector of integers: \", vectorize_layer(tf.expand_dims(X[0], -1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab has the  19999  entries\n",
            "vocab has the following first 10 entries\n",
            "0  represents the word:  \n",
            "1  represents the word:  [UNK]\n",
            "2  represents the word:  the\n",
            "3  represents the word:  .\n",
            "4  represents the word:  ,\n",
            "5  represents the word:  a\n",
            "6  represents the word:  and\n",
            "7  represents the word:  of\n",
            "8  represents the word:  to\n",
            "9  represents the word:  is\n",
            " Given raw data (text):  b'Clint Eastwood has definitely produced better movies than this, but this one does not embarrass him. Dirty Harry catches everyone\\'s attention and unless one wants to watch romance, there is no reason why you won\\'t like him. He is cool because he is dirty, is great because he kills without much thinking, is perfect because he gets the bullet right through your heart and a hero because he doesn\\'t care.<br /><br />From what I have seen in movies in which Eastwood acts, the character of the lead role always captivates the audience. In White Hunter Black heart, he is the crazy director, in \"in the Line of Fire\" he is the \"Old \\'un\" while here is the \"almost\" jobless with his job, that is to say he makes work for himself, doesn\\'t care one damn about his superiors who practically send him out for a vacation.<br /><br />Based on a rape victim, this movie is promising for all the \"no non-sense\" movie watchers. The movie has nothing that goes away from he central plot. However, what makes it slightly inferior to the better movies of Eastwood is that though the character of the lead role is captivating the plot is not, as it is far too obvious from the beginning. It is not a movie that is going to make you sit at a place without moving. Also, there are too many people far dirtier than Dirty harry.'\n",
            " Tokenized and Transformed to a vector of integers:  tf.Tensor(\n",
            "[[ 3491  2620    52   417  1105   137   108    82    13     4    21    13\n",
            "     34   134    28 12009    94     3  1773  1431  4255   307    15   687\n",
            "      6   977    34   498     8   114   911     4    46     9    65   293\n",
            "    149    24   395    26    44    94     3    30     9   621    95    30\n",
            "      9  1773     4     9    90    95    30  1167   217    81   547     4\n",
            "      9   425    95    30   226     2  4163   214   154   136   503     6\n",
            "      5   651    95    30   162    26   466     3]], shape=(1, 80), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CvzcqtU2sFZ"
      },
      "source": [
        "def prepare_pos_lm_inputs_labels(text):\n",
        "    \"\"\"\n",
        "    Shift word sequences by 1 position so that the target for position (i) is\n",
        "    word at position (i+1). The model will use all words up till position (i)\n",
        "    to predict the next word.\n",
        "    \"\"\"\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "\n",
        "    x = tf.squeeze(tokenized_sentences[:, :-1])\n",
        "    s = 1\n",
        "    y = tf.squeeze(tokenized_sentences[:, 1:])\n",
        "    return (x,s), y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIhFd1DSyIRS"
      },
      "source": [
        "text_pos_ds = text_pos_ds_raw.map(prepare_pos_lm_inputs_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUbZlhrfyTWo",
        "outputId": "7741441c-57f1-46af-bcbc-fc00ff835c57"
      },
      "source": [
        "for (X,s),y in text_pos_ds.take(1):\n",
        "  print(\"X.shape: \",X.shape,\"s.shape: \", s.shape, \"y.shape: \", y.shape)\n",
        "  print(\"X: \", X)\n",
        "  print(\"s :\",s)\n",
        "  print(\"y: \",y)\n",
        "  input1 = \" \".join([vocab[_] for _ in X])\n",
        "  input2= s.numpy()\n",
        "  output = \" \".join([vocab[_] for _ in y])\n",
        "  print(\"input1 (in text): \" , input1)\n",
        "  print(\"input2 : \" , input2)\n",
        "  print(\"output (in text): \" , output)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X.shape:  (79,) s.shape:  () y.shape:  (79,)\n",
            "X:  tf.Tensor(\n",
            "[ 3491  2620    52   417  1105   137   108    82    13     4    21    13\n",
            "    34   134    28 12009    94     3  1773  1431  4255   307    15   687\n",
            "     6   977    34   498     8   114   911     4    46     9    65   293\n",
            "   149    24   395    26    44    94     3    30     9   621    95    30\n",
            "     9  1773     4     9    90    95    30  1167   217    81   547     4\n",
            "     9   425    95    30   226     2  4163   214   154   136   503     6\n",
            "     5   651    95    30   162    26   466], shape=(79,), dtype=int64)\n",
            "s : tf.Tensor(1, shape=(), dtype=int32)\n",
            "y:  tf.Tensor(\n",
            "[ 2620    52   417  1105   137   108    82    13     4    21    13    34\n",
            "   134    28 12009    94     3  1773  1431  4255   307    15   687     6\n",
            "   977    34   498     8   114   911     4    46     9    65   293   149\n",
            "    24   395    26    44    94     3    30     9   621    95    30     9\n",
            "  1773     4     9    90    95    30  1167   217    81   547     4     9\n",
            "   425    95    30   226     2  4163   214   154   136   503     6     5\n",
            "   651    95    30   162    26   466     3], shape=(79,), dtype=int64)\n",
            "input1 (in text):  clint eastwood has definitely produced better movies than this , but this one does not embarrass him . dirty harry catches everyone 's attention and unless one wants to watch romance , there is no reason why you won 't like him . he is cool because he is dirty , is great because he kills without much thinking , is perfect because he gets the bullet right through your heart and a hero because he doesn 't care\n",
            "input2 :  1\n",
            "output (in text):  eastwood has definitely produced better movies than this , but this one does not embarrass him . dirty harry catches everyone 's attention and unless one wants to watch romance , there is no reason why you won 't like him . he is cool because he is dirty , is great because he kills without much thinking , is perfect because he gets the bullet right through your heart and a hero because he doesn 't care .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jAo2u0SFoJV"
      },
      "source": [
        "def prepare_neg_lm_inputs_labels(text):\n",
        "    \"\"\"\n",
        "    Shift word sequences by 1 position so that the target for position (i) is\n",
        "    word at position (i+1). The model will use all words up till position (i)\n",
        "    to predict the next word.\n",
        "    \"\"\"\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "\n",
        "    x = tf.squeeze(tokenized_sentences[:, :-1])\n",
        "    #s = batch_size * [0]\n",
        "    s = 0\n",
        "    y = tf.squeeze(tokenized_sentences[:, 1:])\n",
        "    return (x,s), y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9xGHxE6FxZR"
      },
      "source": [
        "text_neg_ds = text_neg_ds_raw.map(prepare_neg_lm_inputs_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dww4LzI7F4py",
        "outputId": "05f064c2-7162-4f0e-aa68-fdfe9a03b0d8"
      },
      "source": [
        "for (X,s),y in text_neg_ds.take(1):\n",
        "  print(\"X.shape: \",X.shape,\"s.shape: \", s.shape, \"y.shape: \", y.shape)\n",
        "  print(\"X: \", X)\n",
        "  print(\"s :\",s)\n",
        "  print(\"y: \",y)\n",
        "  input1 = \" \".join([vocab[_] for _ in X])\n",
        "  input2= s.numpy()\n",
        "  output = \" \".join([vocab[_] for _ in y])\n",
        "  print(\"input1 (in text): \" , input1)\n",
        "  print(\"input2 : \" , input2)\n",
        "  print(\"output (in text): \" , output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X.shape:  (79,) s.shape:  () y.shape:  (79,)\n",
            "X:  tf.Tensor(\n",
            "[   17     5    18  3529    20   454  4789 18951     4    12    32     8\n",
            "    73   177    15     7   108     4     6    28    36    61    57   666\n",
            "     3    56   108    29    43    89     4    24   395    26    33   760\n",
            "    14    38    29   624   742    25   401     3    19  7871     4     1\n",
            "   760    14    10     9   624    25   401    37    36     3    13     9\n",
            "    70   369   412    89   497    89   457  1927     3     5   551     7\n",
            "  7811   310     5   473     7  7871     3], shape=(79,), dtype=int64)\n",
            "s : tf.Tensor(0, shape=(), dtype=int32)\n",
            "y:  tf.Tensor(\n",
            "[    5    18  3529    20   454  4789 18951     4    12    32     8    73\n",
            "   177    15     7   108     4     6    28    36    61    57   666     3\n",
            "    56   108    29    43    89     4    24   395    26    33   760    14\n",
            "    38    29   624   742    25   401     3    19  7871     4     1   760\n",
            "    14    10     9   624    25   401    37    36     3    13     9    70\n",
            "   369   412    89   497    89   457  1927     3     5   551     7  7811\n",
            "   310     5   473     7  7871     3    38], shape=(79,), dtype=int64)\n",
            "input1 (in text):  as a movie critic for several dutch websites , i have to see lot 's of movies , and not all very good ones . some movies are so bad , you won 't be surprised that they are released straight on video . with taboo , [UNK] surprised that it is released on video at all . this is really low budget bad quality bad written rubbish . a group of youngsters plays a game of taboo .\n",
            "input2 :  0\n",
            "output (in text):  a movie critic for several dutch websites , i have to see lot 's of movies , and not all very good ones . some movies are so bad , you won 't be surprised that they are released straight on video . with taboo , [UNK] surprised that it is released on video at all . this is really low budget bad quality bad written rubbish . a group of youngsters plays a game of taboo . they\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgIm7YJtDuUk"
      },
      "source": [
        "all_text_ds = text_pos_ds.concatenate(text_neg_ds)\n",
        "all_text_ds=all_text_ds.shuffle(buffer_size=250000)\n",
        "all_text_ds=all_text_ds.batch(batch_size=batch_size)\n",
        "all_text_ds=all_text_ds.cache()\n",
        "all_text_ds = all_text_ds.prefetch(tf.data.experimental.AUTOTUNE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EfBMnGeAtRz",
        "outputId": "b4012bcf-d4bb-49b7-ac6e-f7f66f301e68"
      },
      "source": [
        "for (X,s), y in all_text_ds.take(1):\n",
        "  print(X.shape, s.shape, y.shape)\n",
        "  print(\"All sentiment values in this batch: \", s)\n",
        "  print(\"\\nFirst sample in the batch:\")\n",
        "  print(\"\\tX is: \" ,X[0])\n",
        "  print(\"\\ts is: \", s[0])\n",
        "  print(\"\\ty is: \", y[0])\n",
        "  input1 = \" \".join([vocab[_] for _ in X[0]])\n",
        "  input2= s[0].numpy()\n",
        "  output = \" \".join([vocab[_] for _ in y[0]])\n",
        "  print(\"\\tinput1 (in text): \" , input1)\n",
        "  print(\"\\tinput2 : \" , input2)\n",
        "  print(\"\\toutput (in text): \" , output)\n",
        "\n",
        "  print(\"\\nSecond sample in the batch:\")\n",
        "  print(\"\\tX is: \" ,X[1])\n",
        "  print(\"\\ts is: \", s[1])\n",
        "  print(\"\\ty is: \", y[1])\n",
        "  input1 = \" \".join([vocab[_] for _ in X[1]])\n",
        "  input2= s[1].numpy()\n",
        "  output = \" \".join([vocab[_] for _ in y[1]])\n",
        "  print(\"\\tinput1 (in text): \" , input1)\n",
        "  print(\"\\tinput2 : \" , input2)\n",
        "  print(\"\\toutput (in text): \" , output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 79) (128,) (128, 79)\n",
            "All sentiment values in this batch:  tf.Tensor(\n",
            "[0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0\n",
            " 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0\n",
            " 1 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 1], shape=(128,), dtype=int32)\n",
            "\n",
            "First sample in the batch:\n",
            "\tX is:  tf.Tensor(\n",
            "[4555   35   28    5   89  106    8  383   10    3   12  201   13   16\n",
            "  224    4   21  102   12 2051   10   16    5 6473    7    2 7665 1048\n",
            "    7  750 2256    3   12  121  218  750 2256  381   12   16 1203    3\n",
            "   12  218   13   22   60   12   16  903    3    2   22   15 8708 6685\n",
            "  215   88   85 1589   11  136  428   35   28    8  747    2   22   15\n",
            "  776  603   39    2    1    3   13    9    2], shape=(79,), dtype=int64)\n",
            "\ts is:  tf.Tensor(0, shape=(), dtype=int32)\n",
            "\ty is:  tf.Tensor(\n",
            "[  35   28    5   89  106    8  383   10    3   12  201   13   16  224\n",
            "    4   21  102   12 2051   10   16    5 6473    7    2 7665 1048    7\n",
            "  750 2256    3   12  121  218  750 2256  381   12   16 1203    3   12\n",
            "  218   13   22   60   12   16  903    3    2   22   15 8708 6685  215\n",
            "   88   85 1589   11  136  428   35   28    8  747    2   22   15  776\n",
            "  603   39    2    1    3   13    9    2  257], shape=(79,), dtype=int64)\n",
            "\tinput1 (in text):  lol ! not a bad way to start it . i thought this was original , but then i discovered it was a clone of the 1976 remake of king kong . i never saw king kong until i was 15 . i saw this film when i was 9 . the film 's funky disco music will get stuck in your head ! not to mention the film 's theme song by the [UNK] . this is the\n",
            "\tinput2 :  0\n",
            "\toutput (in text):  ! not a bad way to start it . i thought this was original , but then i discovered it was a clone of the 1976 remake of king kong . i never saw king kong until i was 15 . i saw this film when i was 9 . the film 's funky disco music will get stuck in your head ! not to mention the film 's theme song by the [UNK] . this is the worst\n",
            "\n",
            "Second sample in the batch:\n",
            "\tX is:  tf.Tensor(\n",
            "[   12   220    74    13  1140    14  3426   108    77    36    49   221\n",
            "     6   474     4    21 16848    11 14818    27    52  1192    66   233\n",
            "     7   655     3    13    18     9    43   596     3    10     1    32\n",
            "   109  3448   146    50   133     2   391   221   146     3    28     5\n",
            "   690   142   129     2   163  1466    51  1427     3    28    68     5\n",
            "   142   129     2   290     6     2   549  2749     4    28    68     5\n",
            "   596  2749     3     6   254    13    18], shape=(79,), dtype=int64)\n",
            "\ts is:  tf.Tensor(1, shape=(), dtype=int32)\n",
            "\ty is:  tf.Tensor(\n",
            "[  220    74    13  1140    14  3426   108    77    36    49   221     6\n",
            "   474     4    21 16848    11 14818    27    52  1192    66   233     7\n",
            "   655     3    13    18     9    43   596     3    10     1    32   109\n",
            "  3448   146    50   133     2   391   221   146     3    28     5   690\n",
            "   142   129     2   163  1466    51  1427     3    28    68     5   142\n",
            "   129     2   290     6     2   549  2749     4    28    68     5   596\n",
            "  2749     3     6   254    13    18     9], shape=(79,), dtype=int64)\n",
            "\tinput1 (in text):  i always had this concept that korean movies were all about comedy and drama , but \"christmas in autumn \" has changed my point of view . this movie is so simple . it [UNK] have any melodramatic scenes or over the top comedy scenes . not a single scene where the actors cry out loud . not even a scene where the actor and the actress kiss , not even a simple kiss . and yet this movie\n",
            "\tinput2 :  1\n",
            "\toutput (in text):  always had this concept that korean movies were all about comedy and drama , but \"christmas in autumn \" has changed my point of view . this movie is so simple . it [UNK] have any melodramatic scenes or over the top comedy scenes . not a single scene where the actors cry out loud . not even a scene where the actor and the actress kiss , not even a simple kiss . and yet this movie is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzQ4fgAh29Mj"
      },
      "source": [
        "def top_k_sample(logits, k=10):\n",
        "    logits, indices = tf.math.top_k(logits, k=k, sorted=True)\n",
        "    indices = np.asarray(indices).astype(\"int32\")\n",
        "    preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
        "    preds = np.asarray(preds).astype(\"float32\")\n",
        "    return np.random.choice(indices, p=preds)\n",
        "\n",
        "def TextGenerator(model, max_tokens=40, start_prompt = \"this movie is\", sentiment= 1, index_to_word=vocab, top_k=10):\n",
        "    \"\"\"A callback to generate text from a trained model.\n",
        "    1. Feed some starting prompt to the model\n",
        "    2. Predict probabilities for the next token\n",
        "    3. Sample the next token and add it to the next input\n",
        "\n",
        "    Arguments:\n",
        "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
        "        start_tokens: List of integers, the token indices for the starting prompt.\n",
        "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
        "        top_k: Integer, sample from the `top_k` token predictions.\n",
        "        print_every: Integer, print after this many epochs.\n",
        "    \"\"\"\n",
        "    def detokenize(number):\n",
        "        return index_to_word[number]\n",
        "    # Tokenize starting prompt\n",
        "    word_to_index = {}\n",
        "    for index, word in enumerate(vocab):\n",
        "        word_to_index[word] = index\n",
        "\n",
        "\n",
        "    start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
        "    start_tokens = [_ for _ in start_tokens]\n",
        "\n",
        "    num_tokens_generated = 0\n",
        "    tokens_generated = []\n",
        "    while num_tokens_generated <= max_tokens:\n",
        "        pad_len = maxlen - len(start_tokens)\n",
        "        sample_index = len(start_tokens) - 1\n",
        "        if pad_len < 0:\n",
        "            x = start_tokens[:maxlen]\n",
        "            sample_index = maxlen - 1\n",
        "        elif pad_len > 0:\n",
        "            x = start_tokens + [0] * pad_len\n",
        "        else:\n",
        "            x = start_tokens\n",
        "        s= sentiment\n",
        "        x = np.array([x])\n",
        "        s = np.array([s])\n",
        "\n",
        "        y, _ = model.predict((x,s))\n",
        "        sample_token = top_k_sample(y[0][sample_index])\n",
        "        tokens_generated.append(sample_token)\n",
        "        start_tokens.append(sample_token)\n",
        "        num_tokens_generated = len(tokens_generated)\n",
        "    txt = \" \".join(\n",
        "        [detokenize(_) for _ in start_tokens + tokens_generated]\n",
        "    )\n",
        "    print(f\"generated text:\\n{txt}\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDfOMfWl29Ml",
        "outputId": "e69fef7d-b311-4382-92a9-d7a82c667c4f"
      },
      "source": [
        "my_model.fit(all_text_ds, verbose=1, epochs=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "391/391 [==============================] - 83s 161ms/step - loss: 5.5790 - dense_2_loss: 5.5790\n",
            "Epoch 2/3\n",
            "391/391 [==============================] - 66s 168ms/step - loss: 4.7112 - dense_2_loss: 4.7112\n",
            "Epoch 3/3\n",
            "391/391 [==============================] - 66s 169ms/step - loss: 4.4631 - dense_2_loss: 4.4631\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f88485f1c90>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fey0r_dAtvCB",
        "outputId": "062e04ad-3071-47dc-96a6-dc7d070ef77f"
      },
      "source": [
        "TextGenerator(my_model, start_prompt = \"this movie is amazing\", sentiment=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "generated text:\n",
            "this movie is amazing ! ! ! a great action movie . the actors don 't get any good action and it looks to make a good movie . the characters are just awful . there 's nothing else on the plot and the main ! ! ! a great action movie . the actors don 't get any good action and it looks to make a good movie . the characters are just awful . there 's nothing else on the plot and the main\n",
            "\n"
          ]
        }
      ]
    }
  ]
}